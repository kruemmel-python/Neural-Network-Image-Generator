{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMgHjR5zP4OpfRcFWDRsp7Z",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kruemmel-python/Neural-Network-Image-Generator/blob/main/imagegenerator.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 663
        },
        "id": "5KMaPFDbpYfh",
        "outputId": "3ee78b44-5d6b-4e3b-e9fb-ae44c2a96b17"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Verwendetes Gerät: cpu\n",
            "Running Gradio in a Colab notebook requires sharing enabled. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://06e47ae401cce51a64.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://06e47ae401cce51a64.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "import gradio as gr\n",
        "import numpy as np\n",
        "import torch\n",
        "import random\n",
        "import cv2\n",
        "import time\n",
        "from PIL import Image, ImageEnhance\n",
        "import os\n",
        "\n",
        "# Geräteauswahl (CUDA wenn verfügbar)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Verwendetes Gerät: {device}\")\n",
        "\n",
        "# Klasse für die Verbindung zwischen Knoten\n",
        "class Connection:\n",
        "    def __init__(self, target_node, weight=None):\n",
        "        self.target_node = target_node\n",
        "        self.weight = weight if weight is not None else random.uniform(0.1, 1.0)\n",
        "        self.weight_history = []\n",
        "\n",
        "# Klasse für den Knoten im neuronalen Netzwerk\n",
        "class Node:\n",
        "    def __init__(self, label):\n",
        "        self.label = label\n",
        "        self.connections = []\n",
        "        self.activation = 0.0\n",
        "        self.activation_history = []\n",
        "\n",
        "    def add_connection(self, target_node, weight=None):\n",
        "        self.connections.append(Connection(target_node, weight))\n",
        "\n",
        "    def propagate_signal(self, input_signal):\n",
        "        self.activation = max(0, input_signal)  # Keine negativen Aktivierungen\n",
        "        self.activation_history.append(self.activation)\n",
        "        for connection in self.connections:\n",
        "            connection.target_node.activation += self.activation * connection.weight\n",
        "            connection.weight_history.append(connection.weight)\n",
        "\n",
        "# Klasse für den Bildknoten\n",
        "class ImageNode(Node):\n",
        "    def __init__(self, label):\n",
        "        super().__init__(label)\n",
        "        self.image = None\n",
        "\n",
        "    def generate_image(self, category_nodes, original_image, brightness_factor, contrast_factor):\n",
        "        self.image = self.generate_image_from_categories(category_nodes, original_image, brightness_factor, contrast_factor)\n",
        "\n",
        "    def generate_image_from_categories(self, category_nodes, original_image, brightness_factor, contrast_factor):\n",
        "        image_array = np.array(original_image) / 255.0  # Normalisiere Originalbild auf [0, 1]\n",
        "        image_tensor = torch.tensor(image_array, dtype=torch.float32).permute(2, 0, 1).to(device)  # Zu Tensor konvertieren und auf das Gerät verschieben\n",
        "\n",
        "        modified_image_tensor = self.process_image(image_tensor, category_nodes, brightness_factor, contrast_factor)\n",
        "        return modified_image_tensor\n",
        "\n",
        "    def process_image(self, image_tensor, category_nodes, brightness_factor, contrast_factor):\n",
        "        modified_image_tensor = image_tensor.clone()\n",
        "        for x in range(modified_image_tensor.shape[1]):\n",
        "            for y in range(modified_image_tensor.shape[2]):\n",
        "                pixel = modified_image_tensor[:, x, y]\n",
        "                brightness = float(brightness_factor)\n",
        "                contrast = float(contrast_factor)\n",
        "                for node in category_nodes:\n",
        "                   brightness += node.activation * 0.1\n",
        "                   contrast += node.activation * 0.1\n",
        "                pixel = torch.clamp((pixel - 0.5) * contrast + 0.5 + brightness, 0, 1)\n",
        "                modified_image_tensor[:, x, y] = pixel\n",
        "        return modified_image_tensor\n",
        "\n",
        "    def get_color_from_label(self, label):\n",
        "        color_map = {\n",
        "            \"Rot\": torch.tensor([1, 0, 0], device=device),\n",
        "            \"Grün\": torch.tensor([0, 1, 0], device=device),\n",
        "            \"Blau\": torch.tensor([0, 0, 1], device=device),\n",
        "            \"Gelb\": torch.tensor([1, 1, 0], device=device),\n",
        "            \"Cyan\": torch.tensor([0, 1, 1], device=device),\n",
        "            \"Magenta\": torch.tensor([1, 0, 1], device=device)\n",
        "        }\n",
        "        return color_map.get(label, torch.tensor([1, 1, 1], device=device))\n",
        "\n",
        "# Funktion zum Extrahieren der Hauptfarbwerte aus dem Bild\n",
        "def extract_main_colors(image):\n",
        "    image_array = np.array(image)\n",
        "    colors = {}\n",
        "    for i in range(image_array.shape[0]):\n",
        "        for j in range(image_array.shape[1]):\n",
        "            color = tuple(image_array[i, j])\n",
        "            if sum(color) > 30:  # Vermeiden von sehr dunklen Farben\n",
        "                if color in colors:\n",
        "                    colors[color] += 1\n",
        "                else:\n",
        "                    colors[color] = 1\n",
        "    # Sortieren nach Häufigkeit und die häufigsten Farben auswählen\n",
        "    sorted_colors = sorted(colors.items(), key=lambda item: item[1], reverse=True)\n",
        "    main_colors = [color for color, count in sorted_colors[:6]]  # Nehmen Sie die 6 häufigsten Farben\n",
        "\n",
        "    # Skalieren der Farben auf einen helleren Bereich\n",
        "    scaled_colors = [(min(c[0] + 50, 255), min(c[1] + 50, 255), min(c[2] + 50, 255)) for c in main_colors]\n",
        "    return scaled_colors\n",
        "\n",
        "# Funktion zum Erstellen des neuronalen Netzwerks\n",
        "def create_neural_network(main_colors):\n",
        "    category_nodes = [Node(label) for label in [\"Rot\", \"Grün\", \"Blau\", \"Gelb\", \"Cyan\", \"Magenta\"]]\n",
        "    for node in category_nodes:\n",
        "        for target_node in category_nodes:\n",
        "            if node != target_node:\n",
        "                node.add_connection(target_node, weight=random.uniform(0.01, 8.0))\n",
        "    return category_nodes\n",
        "\n",
        "# Funktion zum Speichern des Bildes\n",
        "def save_image(image_tensor, filename, resolution, original_size=None):\n",
        "    resolutions = {\n",
        "        \"HD\": (1280, 720),\n",
        "        \"Full HD\": (1920, 1080),\n",
        "        \"2K\": (2048, 2048),\n",
        "        \"4K\": (5760, 3240),\n",
        "        \"8K\": (10670, 6000),\n",
        "        \"Cover\": (1024, 1024),\n",
        "        \"Original (2K)\": (2048, 2048)\n",
        "    }\n",
        "\n",
        "    if resolution == \"Original (2K)\" and original_size:\n",
        "        width, height = original_size\n",
        "    else:\n",
        "        width, height = resolutions.get(resolution, (1920, 1080))\n",
        "\n",
        "    image = Image.fromarray((image_tensor.cpu().numpy().transpose(1, 2, 0) * 255).astype(np.uint8))\n",
        "    image = image.resize((width, height), Image.Resampling.LANCZOS)\n",
        "    image.save(filename, format='webp', lossless=True, quality=100, dpi=(300, 300))  # Speichern als WEBP mit 300 DPI\n",
        "\n",
        "    # Überprüfe, ob die DPI korrekt gesetzt wurden\n",
        "    with Image.open(filename) as img:\n",
        "        saved_dpi = img.info.get('dpi', None)\n",
        "        print(f\"DPI im gespeicherten Bild: {saved_dpi}\")\n",
        "    print(f\"Bild erfolgreich gespeichert als {filename} mit Auflösung {resolution} und 300 DPI\")\n",
        "\n",
        "\n",
        "\n",
        "# Funktion zum Schärfen des Bildes\n",
        "def sharpen_image(image):\n",
        "    enhancer = ImageEnhance.Sharpness(image)\n",
        "    sharpened_image = enhancer.enhance(1.5)  # Schärfe um 1.5 erhöhen\n",
        "    return sharpened_image\n",
        "\n",
        "# Funktion zum Anpassen von Helligkeit und Kontrast\n",
        "def match_histogram(source, template):\n",
        "    source = cv2.cvtColor(np.array(source), cv2.COLOR_RGB2LAB).astype(\"float32\")\n",
        "    template = cv2.cvtColor(np.array(template), cv2.COLOR_RGB2LAB).astype(\"float32\")\n",
        "\n",
        "    (l, a, b) = cv2.split(source)\n",
        "    (lH, aH, bH) = cv2.split(template)\n",
        "\n",
        "    # Anwenden von CLAHE nur auf den L-Kanal\n",
        "    clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8, 8))\n",
        "    l = clahe.apply((l * 255).astype(np.uint8)) / 255.0\n",
        "\n",
        "    # Anwenden von Histogram-Matching auf die a- und b-Kanäle\n",
        "    a = cv2.equalizeHist((a * 255).astype(np.uint8)) / 255.0\n",
        "    b = cv2.equalizeHist((b * 255).astype(np.uint8)) / 255.0\n",
        "\n",
        "    result = cv2.merge((l, a, b))\n",
        "    result = cv2.cvtColor(result.astype(\"uint8\"), cv2.COLOR_LAB2RGB)\n",
        "\n",
        "    return Image.fromarray(result)\n",
        "\n",
        "def calculate_brightness(image):\n",
        "    image_array = np.array(image).astype(float)\n",
        "    mean_brightness = np.mean(image_array) / 255.0  # Normalisieren auf den Bereich [0, 1]\n",
        "    # Verschieben des Bereichs von [0, 1] auf [-1, 1] für den Slider\n",
        "    return (mean_brightness * 2) -1\n",
        "\n",
        "def calculate_contrast(image):\n",
        "    image_array = np.array(image).astype(float)\n",
        "    mean_brightness = np.mean(image_array)\n",
        "\n",
        "    # Berechnung der Standardabweichung und des Kontrasts basierend auf der Helligkeitsvariation\n",
        "    std_dev = np.std(image_array)\n",
        "    contrast = (std_dev / 128.0)  # Skalierung des Kontrasts auf einen Bereich um 1\n",
        "\n",
        "    # Stärkere Kontraste nach oben, schwächere nach unten anpassen\n",
        "    contrast = (contrast * 1.0) + 1.0\n",
        "    contrast = max(0.5, min(2.0, contrast)) # Begrenzung des Kontrasts\n",
        "\n",
        "    return contrast\n",
        "\n",
        "# Funktion zum Generieren des Bildes (angepasst für Gradio)\n",
        "def generate_and_display_image(image, brightness_factor, contrast_factor, resolution):\n",
        "    if image is None:\n",
        "        return None, None\n",
        "    start_time = time.time()\n",
        "\n",
        "    # Extrahieren der Hauptfarbwerte aus dem Bild\n",
        "    main_colors = extract_main_colors(image)\n",
        "    print(f\"Hauptfarbwerte: {main_colors}\")\n",
        "\n",
        "    # Validierung: Sicherstellen, dass mindestens sechs Farben vorhanden sind\n",
        "    if len(main_colors) < 6 or all(sum(color) < 150 for color in main_colors):\n",
        "        print(\"Hauptfarben zu dunkel oder zu wenige Farben, Standardfarben werden verwendet.\")\n",
        "        main_colors = [(255, 0, 0), (0, 255, 0), (0, 0, 255), (255, 255, 0), (0, 255, 255), (255, 0, 255)]  # Standardfarben\n",
        "\n",
        "    # Konvertiere die Farben in normale Aktivierungswerte zwischen 0 und 1\n",
        "    main_colors = [(r / 255.0, g / 255.0, b / 255.0) for r, g, b in main_colors]\n",
        "\n",
        "    # Erstellen des neuronalen Netzwerks\n",
        "    category_nodes = create_neural_network(main_colors)\n",
        "\n",
        "    # Setze initiale Aktivierungen basierend auf den Hauptfarben\n",
        "    for node, color in zip(category_nodes, main_colors):\n",
        "        node.activation = sum(color) / 3  # Durchschnittliche Helligkeit als Aktivierung\n",
        "        print(f\"Knoten {node.label}: Aktivierung = {node.activation}, Farbe = {color}\")\n",
        "\n",
        "    # Erstellen und Verarbeiten des Bildes\n",
        "    image_node = ImageNode(\"Image\")\n",
        "    image_node.generate_image(category_nodes, image, brightness_factor, contrast_factor)\n",
        "\n",
        "    # Anzeigen des generierten Bildes\n",
        "    generated_image = Image.fromarray((image_node.image.cpu().numpy().transpose(1, 2, 0) * 255).astype(np.uint8))\n",
        "    generated_image = sharpen_image(generated_image)  # Schärfen des Bildes\n",
        "    generated_image = match_histogram(generated_image, image)  # Helligkeit und Kontrast anpassen\n",
        "\n",
        "     # Auflösung anpassen, BEVOR das Bild angezeigt wird\n",
        "    resolutions = {\n",
        "        \"HD\": (1280, 720),\n",
        "        \"Full HD\": (1920, 1080),\n",
        "        \"2K\": (2048, 2048),\n",
        "        \"4K\": (5760, 3240),\n",
        "        \"8K\": (10670, 6000),\n",
        "        \"Cover\": (1024, 1024),\n",
        "        \"Original (2K)\": (2048,2048)\n",
        "    }\n",
        "\n",
        "    if resolution == \"Original (2K)\":\n",
        "        width, height = image.size\n",
        "    else:\n",
        "       width, height = resolutions.get(resolution, (1920, 1080))\n",
        "\n",
        "    generated_image = generated_image.resize((width,height), Image.Resampling.LANCZOS)\n",
        "\n",
        "    end_time = time.time()  # Endzeit messen\n",
        "    generation_time = end_time - start_time\n",
        "    print(f\"Bild erfolgreich generiert und angezeigt. Generierungszeit: {generation_time:.2f} Sekunden\")\n",
        "\n",
        "    save_image(image_node.image, \"kunst.webp\", resolution, original_size=image.size if resolution == \"Original (2K)\" else None) # Speichern als WEBP\n",
        "    return image, \"kunst.webp\"\n",
        "\n",
        "def process_inputs(image, brightness, contrast, resolution):\n",
        "    if image is not None:\n",
        "      return generate_and_display_image(image, brightness, contrast, resolution)\n",
        "    else:\n",
        "      return None, None\n",
        "\n",
        "# Funktion zum Laden des Bildes aus dem Dateisystem\n",
        "def load_image_from_file(file_path):\n",
        "    if os.path.exists(file_path):\n",
        "        return Image.open(file_path)\n",
        "    else:\n",
        "        return None\n",
        "\n",
        "# Gradio Interface erstellen\n",
        "if __name__ == \"__main__\":\n",
        "    with gr.Blocks() as iface:\n",
        "        with gr.Row():\n",
        "            with gr.Column():\n",
        "                input_image = gr.Image(label=\"Eingabebild\")\n",
        "                brightness_slider = gr.Slider(minimum=-1, maximum=1, value=0.0, step=0.05, label=\"Helligkeit\")\n",
        "                contrast_slider = gr.Slider(minimum=0.5, maximum=2.0, value=1.0, step=0.05, label=\"Kontrast\")\n",
        "                resolution_dropdown = gr.Dropdown(choices=[\"HD\", \"2K\", \"Full HD\", \"4K\", \"8K\", \"Cover\", \"Original (2K)\"], value=\"Full HD\", label=\"Auflösung\")\n",
        "                generate_button = gr.Button(\"Generiere Bild\")\n",
        "\n",
        "            with gr.Column():\n",
        "                original_output = gr.Image(label=\"Eingabebild\")\n",
        "                generated_output = gr.Image(label=\"Generiertes Bild\")\n",
        "\n",
        "        input_params = [input_image, brightness_slider, contrast_slider, resolution_dropdown]\n",
        "        output_params = [original_output, generated_output]\n",
        "\n",
        "        generate_button.click(process_inputs, inputs=input_params, outputs=output_params)\n",
        "\n",
        "        # Laden des generierten Bildes aus dem Dateisystem und Anzeigen in der Weboberfläche\n",
        "        generated_image_path = \"kunst.webp\"\n",
        "        generated_image = load_image_from_file(generated_image_path)\n",
        "        if generated_image:\n",
        "            generated_output.value = generated_image\n",
        "\n",
        "    iface.launch()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install gradio\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yB_kz1Utpj2m",
        "outputId": "5df5ed97-957e-411d-8209-5b219732c2b1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting gradio\n",
            "  Downloading gradio-5.9.1-py3-none-any.whl.metadata (16 kB)\n",
            "Collecting aiofiles<24.0,>=22.0 (from gradio)\n",
            "  Downloading aiofiles-23.2.1-py3-none-any.whl.metadata (9.7 kB)\n",
            "Requirement already satisfied: anyio<5.0,>=3.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (3.7.1)\n",
            "Collecting fastapi<1.0,>=0.115.2 (from gradio)\n",
            "  Downloading fastapi-0.115.6-py3-none-any.whl.metadata (27 kB)\n",
            "Collecting ffmpy (from gradio)\n",
            "  Downloading ffmpy-0.5.0-py3-none-any.whl.metadata (3.0 kB)\n",
            "Collecting gradio-client==1.5.2 (from gradio)\n",
            "  Downloading gradio_client-1.5.2-py3-none-any.whl.metadata (7.1 kB)\n",
            "Requirement already satisfied: httpx>=0.24.1 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.28.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.25.1 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.27.0)\n",
            "Requirement already satisfied: jinja2<4.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (3.1.4)\n",
            "Collecting markupsafe~=2.0 (from gradio)\n",
            "  Downloading MarkupSafe-2.1.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.0 kB)\n",
            "Requirement already satisfied: numpy<3.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (1.26.4)\n",
            "Requirement already satisfied: orjson~=3.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (3.10.12)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from gradio) (24.2)\n",
            "Requirement already satisfied: pandas<3.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.2.2)\n",
            "Requirement already satisfied: pillow<12.0,>=8.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (11.0.0)\n",
            "Requirement already satisfied: pydantic>=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.10.3)\n",
            "Collecting pydub (from gradio)\n",
            "  Downloading pydub-0.25.1-py2.py3-none-any.whl.metadata (1.4 kB)\n",
            "Collecting python-multipart>=0.0.18 (from gradio)\n",
            "  Downloading python_multipart-0.0.20-py3-none-any.whl.metadata (1.8 kB)\n",
            "Requirement already satisfied: pyyaml<7.0,>=5.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (6.0.2)\n",
            "Collecting ruff>=0.2.2 (from gradio)\n",
            "  Downloading ruff-0.8.5-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (25 kB)\n",
            "Collecting safehttpx<0.2.0,>=0.1.6 (from gradio)\n",
            "  Downloading safehttpx-0.1.6-py3-none-any.whl.metadata (4.2 kB)\n",
            "Collecting semantic-version~=2.0 (from gradio)\n",
            "  Downloading semantic_version-2.10.0-py2.py3-none-any.whl.metadata (9.7 kB)\n",
            "Collecting starlette<1.0,>=0.40.0 (from gradio)\n",
            "  Downloading starlette-0.45.1-py3-none-any.whl.metadata (6.3 kB)\n",
            "Collecting tomlkit<0.14.0,>=0.12.0 (from gradio)\n",
            "  Downloading tomlkit-0.13.2-py3-none-any.whl.metadata (2.7 kB)\n",
            "Requirement already satisfied: typer<1.0,>=0.12 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.15.1)\n",
            "Requirement already satisfied: typing-extensions~=4.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (4.12.2)\n",
            "Collecting uvicorn>=0.14.0 (from gradio)\n",
            "  Downloading uvicorn-0.34.0-py3-none-any.whl.metadata (6.5 kB)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from gradio-client==1.5.2->gradio) (2024.10.0)\n",
            "Requirement already satisfied: websockets<15.0,>=10.0 in /usr/local/lib/python3.10/dist-packages (from gradio-client==1.5.2->gradio) (14.1)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5.0,>=3.0->gradio) (3.10)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<5.0,>=3.0->gradio) (1.3.1)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5.0,>=3.0->gradio) (1.2.2)\n",
            "Collecting starlette<1.0,>=0.40.0 (from gradio)\n",
            "  Downloading starlette-0.41.3-py3-none-any.whl.metadata (6.0 kB)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx>=0.24.1->gradio) (2024.12.14)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx>=0.24.1->gradio) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx>=0.24.1->gradio) (0.14.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.25.1->gradio) (3.16.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.25.1->gradio) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.25.1->gradio) (4.67.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas<3.0,>=1.0->gradio) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas<3.0,>=1.0->gradio) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas<3.0,>=1.0->gradio) (2024.2)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.0->gradio) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.1 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.0->gradio) (2.27.1)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0,>=0.12->gradio) (8.1.7)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0,>=0.12->gradio) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0,>=0.12->gradio) (13.9.4)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas<3.0,>=1.0->gradio) (1.17.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (2.18.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.25.1->gradio) (3.4.0)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.25.1->gradio) (2.2.3)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio) (0.1.2)\n",
            "Downloading gradio-5.9.1-py3-none-any.whl (57.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.2/57.2 MB\u001b[0m \u001b[31m13.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading gradio_client-1.5.2-py3-none-any.whl (320 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m320.4/320.4 kB\u001b[0m \u001b[31m20.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading aiofiles-23.2.1-py3-none-any.whl (15 kB)\n",
            "Downloading fastapi-0.115.6-py3-none-any.whl (94 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m94.8/94.8 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading MarkupSafe-2.1.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (25 kB)\n",
            "Downloading python_multipart-0.0.20-py3-none-any.whl (24 kB)\n",
            "Downloading ruff-0.8.5-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.3/11.3 MB\u001b[0m \u001b[31m96.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading safehttpx-0.1.6-py3-none-any.whl (8.7 kB)\n",
            "Downloading semantic_version-2.10.0-py2.py3-none-any.whl (15 kB)\n",
            "Downloading starlette-0.41.3-py3-none-any.whl (73 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m73.2/73.2 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tomlkit-0.13.2-py3-none-any.whl (37 kB)\n",
            "Downloading uvicorn-0.34.0-py3-none-any.whl (62 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.3/62.3 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ffmpy-0.5.0-py3-none-any.whl (6.0 kB)\n",
            "Downloading pydub-0.25.1-py2.py3-none-any.whl (32 kB)\n",
            "Installing collected packages: pydub, uvicorn, tomlkit, semantic-version, ruff, python-multipart, markupsafe, ffmpy, aiofiles, starlette, safehttpx, gradio-client, fastapi, gradio\n",
            "  Attempting uninstall: markupsafe\n",
            "    Found existing installation: MarkupSafe 3.0.2\n",
            "    Uninstalling MarkupSafe-3.0.2:\n",
            "      Successfully uninstalled MarkupSafe-3.0.2\n",
            "Successfully installed aiofiles-23.2.1 fastapi-0.115.6 ffmpy-0.5.0 gradio-5.9.1 gradio-client-1.5.2 markupsafe-2.1.5 pydub-0.25.1 python-multipart-0.0.20 ruff-0.8.5 safehttpx-0.1.6 semantic-version-2.10.0 starlette-0.41.3 tomlkit-0.13.2 uvicorn-0.34.0\n"
          ]
        }
      ]
    }
  ]
}